ğŸ“¥ Consumidores Kafka â€“ Capa de IngestiÃ³n

Este mÃ³dulo contiene los scripts responsables de leer y procesar los mensajes enviados por los productores Kafka de la capa de Fuente de Datos.

Incluye consumidores individuales para cada tipo de sensor y un consumidor unificado para debugging.

ğŸ“Œ Consumidores actuales
âœ… 1. consumidor_calidad_aire.py

Lee mensajes del topic:

datos_calidad_aire


Ejecutar:

python consumidor_calidad_aire.py

âœ… 2. consumidor_sonido.py

Lee mensajes del topic:

datos_sonido


Ejecutar:

python consumidor_sonido.py

âœ… 3. consumidor_soterrado.py

Lee mensajes del topic:

datos_soterrado


Ejecutar:

python consumidor_soterrado.py

ğŸš€ 4. consumidor_multitopics.py

Este consumidor escucha los 3 topics simultÃ¡neamente:

datos_calidad_aire

datos_sonido

datos_soterrado

Ideal para debugging, cuando los 3 productores estÃ¡n enviando datos a la vez.

Ejecutar:

python consumidor_multitopics.py


Ejemplo de salida:

ğŸ“Œ Topic: datos_sonido
{
  "time": "2025-11-12T12:57:30.179151+00:00",
  "deviceInfo": { "deviceName": "Sensor_Sonido_3" },
  "object": {
    "laeq": 74.5,
    "lai": 63.1,
    "laiMax": 101.2,
    "battery": 88,
    "status": "OK"
  }
}
------------------------------------------------------------

ğŸ†• ActualizaciÃ³n importante (productores basados en CSV)

Desde la Ãºltima actualizaciÃ³n, los productores fueron mejorados para:

âœ” Leer datos reales desde archivos CSV

Ubicados en la carpeta:

C:/datosBigData/


Ejemplo de estructura:

C:/datosBigData/
 â”œâ”€â”€ datos_aire/
 â”œâ”€â”€ datos_sonido/
 â””â”€â”€ datos_soterrado/

âœ” Enviar archivos completos de golpe si son pocos

Si un CSV tiene menos de 500 registros, se envÃ­a todo en un solo lote.

âœ” Enviar en lotes si el archivo es grande

Si tiene miles de filas, los productores envÃ­an los datos asÃ­:

500 mensajes por lote
pausa de 0.2s
siguiente lote...


Esto evita saturar Kafka y mantiene un flujo estable.

âœ” Caer a modo aleatorio si el CSV no existe

En caso de que la carpeta no tenga archivos CSV vÃ¡lidos:

â†’ Se generan datos aleatorios cada 2 segundos
â†’ Para pruebas rÃ¡pidas sin dataset real

ğŸ³ Requisitos

Antes de ejecutar cualquier consumidor o productor, asegÃºrate de que Kafka estÃ© funcionando.

1ï¸âƒ£ Iniciar Kafka + Zookeeper
docker-compose up -d

2ï¸âƒ£ Verificar que los topics existen
docker exec kafka bash -c "/usr/bin/kafka-topics --list --bootstrap-server localhost:9092"


DeberÃ­as ver:

datos_calidad_aire
datos_sonido
datos_soterrado

3ï¸âƒ£ Ejecutar un productor correspondiente

Ejemplos:

python productor_calidad_aire_csv.py
python productor_sonido_csv.py
python productor_soterrado_csv.py


Solo entonces los consumidores empezarÃ¡n a mostrar mensajes.

ğŸ’¡ Notas importantes

Cada consumidor usa un group_id diferente â†’ no interfieren entre sÃ­

Los consumidores se mantienen para:

pruebas locales

debugging

validaciÃ³n de topics

En la capa de Procesamiento, usarÃ¡n Spark Structured Streaming como consumidor principal.

ğŸ“ Historial de cambios relevantes
âœ” Reemplazo de productores aleatorios por productores CSV

Fecha: 16/11/2025

Lectura directa desde C:/datosBigData

EnvÃ­o en lotes

Formato JSON estructurado

âœ” CreaciÃ³n de consumidor multitopic

Fecha: 12/11/2025

Ideal para observar mÃºltiples flujos simultÃ¡neamente

âœ” ConfiguraciÃ³n completa de Kafka con Docker Compose

Fecha: 12/11/2025

MigraciÃ³n exitosa a confluentinc/cp-kafka

Topics creados correctamente

Consumidores funcionales