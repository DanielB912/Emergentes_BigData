# ğŸ“¡ Proyecto Big Data â€“ Kafka + Spark Streaming + Simuladores de Sensores

Este proyecto implementa un pipeline de Big Data donde:

- Sensores simulados (aire, sonido, soterrado) envÃ­an datos en JSON.
- Los datos se publican en Kafka en 3 topics.
- Spark Structured Streaming consume los mensajes en tiempo real usando `spark-submit`.

---

# ğŸš€ 1. Requisitos previos

### âœ” Docker Desktop  
### âœ” Python 3.10+  
### âœ” LibrerÃ­as Python necesarias

```bash
pip install kafka-python pandas pyspark
ğŸ“¦ 2. Estructura del Proyecto
markdown
Copiar cÃ³digo
BackEnd_Equipo3/
â”‚
â”œâ”€â”€ datos_sensores/
â”‚   â”œâ”€â”€ calidad_aire/
â”‚   â”œâ”€â”€ sonido/
â”‚   â””â”€â”€ soterrado/
â”‚
â”œâ”€â”€ kafka-docker/
â”‚   â””â”€â”€ docker-compose.yml   â†’ Kafka + Zookeeper
â”‚
â”œâ”€â”€ Procesamiento/
â”‚   â”œâ”€â”€ docker-compose.yml   â†’ Spark Master + Workers + Spark-submit
â”‚   â”œâ”€â”€ Dockerfile.spark
â”‚   â”œâ”€â”€ spark/
â”‚   â”‚   â””â”€â”€ spark_consumer_kafka.py  â† CONSUMIDOR REAL
â”‚   â””â”€â”€ docs
â”‚
â””â”€â”€ producers/
    â”œâ”€â”€ productor_calidad_aire.py
    â”œâ”€â”€ productor_sonido.py
    â””â”€â”€ productor_soterrado.py
ğŸŸ¦ 3. Levantar Kafka
bash
Copiar cÃ³digo
cd BackEnd_Equipo3/kafka-docker
docker-compose up -d
Verificar:

bash
Copiar cÃ³digo
docker ps
Crear los topics (una sola vez):

bash
Copiar cÃ³digo
docker exec kafka bash -c "/usr/bin/kafka-topics --create --topic datos_calidad_aire --bootstrap-server kafka:9092 --partitions 1 --replication-factor 1"
docker exec kafka bash -c "/usr.bin/kafka-topics --create --topic datos_sonido --bootstrap-server kafka:9092 --partitions 1 --replication-factor 1"
docker exec kafka bash -c "/usr.bin/kafka-topics --create --topic datos_soterrado --bootstrap-server kafka:9092 --partitions 1 --replication-factor 1"
ğŸ”§ 4. Levantar Spark (Master + Workers + Spark-submit)
Crear la red externa (solo una vez):

bash
Copiar cÃ³digo
docker network create bigdata_net
Levantar Spark:

bash
Copiar cÃ³digo
cd BackEnd_Equipo3/Procesamiento
docker-compose up -d --build
Interfaces disponibles:

Master UI â†’ http://localhost:8080

Workers UI â†’ http://localhost:8081 y http://localhost:8082

ğŸ“ˆ 5. Ejecutar los productores Python (simular sensores)
En 3 terminales separados:

bash
Copiar cÃ³digo
cd BackEnd_Equipo3/producers
python productor_calidad_aire.py
python productor_sonido.py
python productor_soterrado.py
ğŸ”¥ 6. Ejecutar el Consumidor Spark (REAL)
Este es el paso CORRECTO que hemos usado:

Entrar al contenedor spark-submit:
bash
Copiar cÃ³digo
docker exec -it spark-submit bash
Ejecutar Spark Streaming con Kafka:
bash
Copiar cÃ³digo
spark-submit \
  --master spark://spark-master:7077 \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \
  spark_consumer_kafka.py
DeberÃ­as ver:

markdown
Copiar cÃ³digo
-----------------------------------------
Batch: 32
-----------------------------------------
| time | deviceInfo | object |
Cada batch contiene los datos de los sensores que llegan de Kafka.