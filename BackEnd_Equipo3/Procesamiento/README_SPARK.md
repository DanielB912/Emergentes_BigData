\# ğŸ§© Proyecto Big Data â€“ Kafka + Spark + Productores Python



Este entorno permite simular flujos de datos IoT usando \*\*Kafka\*\* como broker de mensajerÃ­a, \*\*Spark Structured Streaming\*\* como consumidor en tiempo real y \*\*PostgreSQL\*\* como base de datos de destino.  

Incluye tres productores en Python que generan datos para diferentes tÃ³picos.



---



\## ğŸ“‚ Estructura del Proyecto



Emergentes\_BigData/

â”‚

â”œâ”€ BackEnd\_Equipo3/

â”‚ â”œâ”€ kafka-docker/ # Docker Compose para Kafka + Zookeeper

â”‚ â”‚ â””â”€ docker-compose.yml

â”‚ â”‚

â”‚ â”œâ”€ Procesamiento/ # Docker Compose para Spark + Postgres

â”‚ â”‚ â”œâ”€ docker-compose.yml

â”‚ â”‚ â”œâ”€ Dockerfile.spark

â”‚ â”‚ â””â”€ spark/

â”‚ â”‚ â””â”€ spark\_consumer\_kafka.py

â”‚ â”‚

â”‚ â”œâ”€ producers/ # Productores Python (sensores simulados)

â”‚ â”‚ â”œâ”€ productor\_calidad\_aire.py

â”‚ â”‚ â”œâ”€ productor\_sonido.py

â”‚ â”‚ â””â”€ productor\_soterrado.py

â”‚ â”‚

â”‚ â””â”€ datos\_sensores/

â”‚ â”œâ”€ calidad\_aire/

â”‚ â”œâ”€ sonido/

â”‚ â””â”€ soterrado/



yaml

Copiar cÃ³digo



---



\## ğŸš€ 1. Requisitos Previos



AsegÃºrate de tener instalado:



\- \[Docker Desktop](https://www.docker.com/products/docker-desktop/)

\- \[Python 3.11+](https://www.python.org/downloads/)

\- LibrerÃ­as necesarias:

&nbsp; ```bash

&nbsp; pip install kafka-python pandas pyspark

âš™ï¸ 2. Crear Red Compartida

Permite que los contenedores de Kafka y Spark se comuniquen.



bash

Copiar cÃ³digo

docker network create bigdata\_net

ğŸ§± 3. Levantar Kafka y Zookeeper

Desde la carpeta:



bash

Copiar cÃ³digo

cd BackEnd\_Equipo3\\kafka-docker

docker-compose up -d

Verifica los contenedores:



bash

Copiar cÃ³digo

docker ps

DeberÃ­as ver zookeeper y kafka activos.



ğŸ“¡ Crear Topics

bash

Copiar cÃ³digo

docker exec kafka bash -c "/usr/bin/kafka-topics --create --topic datos\_calidad\_aire --bootstrap-server localhost:9093 --partitions 1 --replication-factor 1"

docker exec kafka bash -c "/usr/bin/kafka-topics --create --topic datos\_sonido --bootstrap-server localhost:9093 --partitions 1 --replication-factor 1"

docker exec kafka bash -c "/usr/bin/kafka-topics --create --topic datos\_soterrado --bootstrap-server localhost:9093 --partitions 1 --replication-factor 1"



docker exec kafka bash -c "/usr/bin/kafka-topics --list --bootstrap-server localhost:9093"

ğŸ 4. Ejecutar Productores Python

Cada productor envÃ­a datos simulados a su topic correspondiente.



Crear entorno virtual (opcional)

bash

Copiar cÃ³digo

cd C:\\Emergentes\_BigData

python -m venv .venv

.\\.venv\\Scripts\\Activate.ps1

pip install kafka-python pandas

Ejecutar productores

bash

Copiar cÃ³digo

\# Desde PowerShell (cada uno en una terminal aparte)

$env:KAFKA\_BROKER\_URL="localhost:9093"



python .\\BackEnd\_Equipo3\\producers\\productor\_calidad\_aire.py

python .\\BackEnd\_Equipo3\\producers\\productor\_sonido.py

python .\\BackEnd\_Equipo3\\producers\\productor\_soterrado.py

VerÃ¡s en consola:



css

Copiar cÃ³digo

Enviado: {"time": "...", "deviceInfo": {...}, "object": {...}}

âš™ï¸ 5. Levantar el Entorno Spark + Postgres

Desde la carpeta de procesamiento:



bash

Copiar cÃ³digo

cd BackEnd\_Equipo3\\Procesamiento

docker-compose up -d --build

Verifica contenedores:



bash

Copiar cÃ³digo

docker ps

Interfaces web:



Spark Master UI â†’ http://localhost:8080



Spark Worker 1 UI â†’ http://localhost:8081



ğŸ”¥ 6. Ejecutar el Consumidor Spark (lee de Kafka)

Entra al contenedor spark-submit:



bash

Copiar cÃ³digo

docker exec -it spark-submit bash

cd /opt/spark-apps

Ejecuta el consumidor:



bash

Copiar cÃ³digo

spark-submit \\

&nbsp; --master spark://spark-master:7077 \\

&nbsp; --packages org.apache.spark:spark-sql-kafka-0-10\_2.12:3.5.1 \\

&nbsp; spark\_consumer\_kafka.py

âœ… Spark se conectarÃ¡ al broker kafka:9092 (listener interno)

y comenzarÃ¡ a mostrar los mensajes en tiempo real.



ğŸ—„ï¸ 7. (Opcional) Guardar en PostgreSQL

Para almacenar los datos procesados:



En el archivo spark\_consumer\_kafka.py, reemplaza la salida en consola por:



python

Copiar cÃ³digo

def save\_to\_postgres(batch\_df, batch\_id):

&nbsp;   batch\_df.write \\

&nbsp;       .format("jdbc") \\

&nbsp;       .option("url", "jdbc:postgresql://pg:5432/gamc") \\

&nbsp;       .option("dbtable", "datos\_iot") \\

&nbsp;       .option("user", "postgres") \\

&nbsp;       .option("password", "postgres") \\

&nbsp;       .mode("append") \\

&nbsp;       .save()



query = df.writeStream \\

&nbsp;   .outputMode("append") \\

&nbsp;   .foreachBatch(save\_to\_postgres) \\

&nbsp;   .start()

ğŸ§  8. Flujo Completo del Sistema

scss

Copiar cÃ³digo

&nbsp;  ğŸ Productores Python (simulan sensores)

&nbsp;              â”‚

&nbsp;              â–¼

&nbsp;        ğŸ“¬ Apache Kafka (Topics)

&nbsp;              â”‚

&nbsp;              â–¼

&nbsp;     âš™ï¸ Apache Spark Streaming (Consumidor)

&nbsp;              â”‚

&nbsp;              â–¼

&nbsp;          ğŸ—„ï¸ PostgreSQL (Almacenamiento)

âœ… 9. Apagar el entorno

Cuando termines:



bash

Copiar cÃ³digo

\# En kafka-docker

docker-compose down



\# En procesamiento

docker-compose down

ğŸ§© 10. Recursos Ãºtiles

Apache Kafka Docs



Apache Spark Structured Streaming



PostgreSQL Docs



Confluent Kafka Images

