# Java 17 para Spark 3.5.x
# FROM eclipse-temurin:17-jdk

# # Python + libs Ãºtiles
# RUN apt-get update && apt-get install -y curl python3 python3-pip && \
#     pip3 install --break-system-packages pyspark pandas kafka-python psycopg2-binary && \
#     apt-get clean

# # Instalar Spark 3.5.1 Hadoop 3
# RUN curl -L -o spark.tgz https://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz && \
#     tar -xzf spark.tgz -C /opt/ && \
#     mv /opt/spark-3.5.1-bin-hadoop3 /opt/spark && \
#     rm spark.tgz

# ENV SPARK_HOME=/opt/spark
# ENV PATH=$SPARK_HOME/bin:$PATH
# ENV PYSPARK_PYTHON=python3
# ENV PYSPARK_DRIVER_PYTHON=python3

# WORKDIR /opt/spark-apps
# CMD ["/bin/bash"]


FROM python:3.10-bullseye

# Instalar dependencias de Java (Spark lo necesita)
RUN apt-get update && apt-get install -y openjdk-17-jdk curl wget procps

# Descargar Spark 3.5.0 desde ARCHIVE (no se borra nunca)
RUN wget https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz \
    && tar -xvzf spark-3.5.0-bin-hadoop3.tgz -C /opt \
    && mv /opt/spark-3.5.0-bin-hadoop3 /opt/spark \
    && rm spark-3.5.0-bin-hadoop3.tgz

ENV SPARK_HOME=/opt/spark
ENV PATH="$SPARK_HOME/bin:$PATH"

WORKDIR /opt/app

CMD ["bash"]



